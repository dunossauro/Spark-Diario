# Spark-Diario
Notas de aprendizado sobre Spark, usando python e HDFS, na Fatec Americana.

#### HDFS
Este projeto faz uso do HDFS, Hadoop Distributed File System, e sua instalação e projetos anteriores se encontram no repositório:

[Hadoop Diario](https://github.com/z4r4tu5tr4/Hadoop-diario)

#### MapReduceLib
Para facilitar o acesso a dados do hdfs, e também outras funções do Hadoop, foi criada um mini-biblioteca para fazer interações do PySpark

[MapReduceLib](https://github.com/z4r4tu5tr4/MapReduceLib)

#### Spark on Yarn
Para usar o gerente de cluster do Hadoop, YARN, foram feitas algumas alterações na configuração do spark disponíveis em:

[Spark on Yarn](https://github.com/z4r4tu5tr4/Spark-Diario/tree/master/spark_on_yarn)

#### Jupyter
Para uso do PySpark no jupyter, que possívelmente aparecerá em alguns prints, foram feitas as seguintes configurações:

[PySpark on Jupyter]()

#### As configurações, versões e metodo usádo para instalação estão disponíveis em:

[Sobre o Spark-cluster]()

#### Sobre este diretório

Ele nasceu com objetivo de aprender e ensinar Apache Spark a todos os interessados e com algumas prioridades:

	1. Fazer com que o aprendizado de Spark seja acessivel a todos os não falantes de inglês ou de Sparkes
	2. Propagar o uso de Software Livre
	3. Usar Python o máximo possível
	4. Resolver alguns problemas de linguistica de corpus
